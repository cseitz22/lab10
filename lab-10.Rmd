---
title: "Lab 10 - Grading the professor, Pt. 2"
author: "Cat Seitz"
date: "03.09.2023"
output: github_document
---

### Load packages and data

```{r load-packages, message=FALSE}
library(tidyverse) 
library(tidymodels)
library(openintro)
library(broom)
```

```{r see-data}

evals<-evals

```


### Exercise 1

```{r fit-linear-model-beauty}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals) %>%
  tidy()

summary(lm(score ~ bty_avg, data = evals))

```

Model: (Evaluation Score) = 3.88 + 0.067(Beauty Rating)

The R2 was .035 and the adjusted R2 was .033

### Exercise 2

```{r multi-reg1}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + gender, data = evals) %>%
  tidy()

summary(lm(score ~ bty_avg + gender, data = evals))

```

Model: Evaluation Score = 3.75 + 0.074(Beauty Rating) + 0.17(Gender)

### Exercise 3

The evaluation score of a female with a beauty rating of 0 is 3.75. With each point increase in beauty rating, the model predicts a professor can add .074 points to their evaluation score. The model also predicts that males have evaluation scores .17 points higher than females. 

### Exercise 4

Roughly 5.5% of variability in the professors' evaluation scores is explained by the model. 

### Exercise 5

Line for just males: Evaluation score = 3.92 + 0.074(Beauty Rating)

### Exercise 6

Males tend to have higher evaluation scores given the same beauty rating. 

### Exercise 7

```{r vary-by-gender}

model<- lm(score ~ bty_avg * gender, data=evals)

evals<- evals%>% 
  mutate(fitted_values=fitted(model))

ggplot(evals, aes(y = score, x=bty_avg, color=gender))+
  geom_jitter()+
  geom_line(aes(y=fitted_values))+
  #stat_smooth(method="lm", color= "orange", se=FALSE)+
  labs(title="Relationship between Score and Beauty", y="Evaluation Score", x="Average Beauty Rating")



```

Based on this plot, it looks like the beauty rating matters less for females than it does for males. 

```{r gender-separation}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals %>% filter(gender=="female")) %>%
  tidy()

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals %>% filter(gender=="male")) %>%
  tidy()

```

Running these models again and filtering the data for each gender, a 1 point increase in beauty rating for females increases the evaluation score by only 0.03; while for males, it increases the evaluation score by 0.11. 

### Exercise 8 

The adjusted R2 increased by 0.022, suggesting gender is useful for explaining some of the variance in evaluation scores. 

### Exercise 9

The slope for m_bty_gen is higher than for m_bty, so adding gender to the model has changed the parameter estimate for the beuaty rating. 

### Exercise 10

```{r multi-reg2}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + rank, data = evals) %>%
  tidy()

summary(lm(score ~ bty_avg + rank, data = evals))

```


Model: Evaluation Score = 3.98 + 0.068(Beauty Rating) - 0.16(Tenure Track) - 0.13(Tenured)

The intercept indicates the evaluation score of a teaching professor with a beauty rating of 0 (which is very unlikely). With each increase in point of beauty rating, the predicted evaluation score increases by .068 points. When predicting the score of a tenure track professor, the evaluation score decreases by .16 and of a tenured professor, decreases by .13. 

### Exercise 11

I would expect the number of students in the class who completed the evaluation to be the worst predictor of evaluation score because it seems completely irrelevant. At first, I was thinking percent of student who completed the evaluation to also be irrelevant, which still may be true, but I might expect that a higher percentage of evals completed to be from professors who are really good or really bad. Lastly, I also wouldn't necessarily suspect that the total number of students in the class, the number of professors to teach the class, or the number of credits to effect the evaluation score either, but there may be indirect reasons these would effect the rating. 

### Exercise 12

```{r num-students-model}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ cls_did_eval, data = evals) %>%
  tidy()

summary(lm(score ~ cls_did_eval, data = evals))

```

Model: Evaluation Score = 4.15 + 0.0008(did eval)

Seems like my suspicion was pretty good because the number of students in the class who completed the evaluation would be a horrible predictor of the evaluation score. 

### Exercise 13

We shouldn't include cls_did_eval if we are already including cls_perc_eval and cls_students because then we would be redundant. 

### Exercise 14

```{r full-model}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ rank + bty_avg + ethnicity + gender + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits, data = evals) %>%
  tidy()

summary(lm(score ~ rank + bty_avg + ethnicity + gender + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits, data = evals))

```

### Exercise 15

```{r final-model}

linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + ethnicity + gender + cls_credits, data = evals) %>%
  tidy()

summary(lm(score ~ bty_avg + ethnicity + gender + cls_credits, data = evals))

```

Using backward-selection, I've come to the following model: 

Evaluation score = 3.51 + 0.082(Beauty rating) + 0.21(Ethnicity) + 0.15(Gender/Male) + 0.58(Number of Credits/1 credit)












